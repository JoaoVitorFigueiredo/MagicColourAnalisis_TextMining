{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d303197-4d96-42dd-aeba-592596541646",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "# Função de pré-processamento, tokenização e remoção de stop words\n",
    "def preprocess(text, stop_words):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text.lower())  # Remove pontuação e converte para minúsculas\n",
    "    tokens = word_tokenize(text)  # Tokeniza o texto\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]  # Remove as stop words\n",
    "    return filtered_tokens\n",
    "\n",
    "# Função para contar coocorrência\n",
    "def count_coocurrences(corpus, window_size=1):\n",
    "    coocurrence = Counter()\n",
    "    for doc in corpus:\n",
    "        for i, word in enumerate(doc):\n",
    "            for j in range(i + 1, min(i + window_size + 1, len(doc))):\n",
    "                coocurrence[(word, doc[j])] += 1\n",
    "    return coocurrence\n",
    "\n",
    "# Calcular o PMI para um par de palavras\n",
    "def calculate_pmi(word1, word2, coocurrence, word_freq, corpus_size):\n",
    "    coocurrence_count = coocurrence.get((word1, word2), 0)\n",
    "    p_word1 = word_freq[word1] / corpus_size\n",
    "    p_word2 = word_freq[word2] / corpus_size\n",
    "    p_word1_word2 = coocurrence_count / corpus_size\n",
    "    return np.log(p_word1_word2 / (p_word1 * p_word2)) if p_word1_word2 > 0 else 0\n",
    "\n",
    "# Construir a árvore de palavras para uma cor\n",
    "def build_color_tree(corpus, color_words, stop_words):\n",
    "    tokenized_text = corpus.apply(lambda x: preprocess(x, stop_words))  # Tokenizar e remover stopwords\n",
    "    word_freq_text = Counter(word for doc in tokenized_text for word in doc)  # Contar frequências\n",
    "    coocurrence_text = count_coocurrences(tokenized_text)  # Contar coocorrências\n",
    "    \n",
    "    corpus_size_text = sum(word_freq_text.values())  # Tamanho do corpus\n",
    "    tree = {color: [] for color in color_words}  # Estrutura da árvore para cores\n",
    "    \n",
    "    # Calcular o PMI para as palavras e construir a árvore\n",
    "    for color in color_words:\n",
    "        for word, count in word_freq_text.items():\n",
    "            pmi_value = calculate_pmi(word, color, coocurrence_text, word_freq_text, corpus_size_text)\n",
    "            if pmi_value > 0:  # Se o PMI for maior que zero, adicionamos à árvore\n",
    "                tree[color].append((word, pmi_value))\n",
    "        \n",
    "        # Ordenar as palavras por PMI (maior PMI é mais próximo)\n",
    "        tree[color] = sorted(tree[color], key=lambda x: x[1], reverse=True)\n",
    "    return tree\n",
    "\n",
    "\n",
    "\n",
    "# Definir palavras associadas às cores\n",
    "color_words = ['change']\n",
    "\n",
    "# Definir stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Construir a árvore de palavras para as cores\n",
    "color_tree = build_color_tree(df['flavor'], color_words, stop_words)\n",
    "\n",
    "# Exibir a árvore com as palavras mais próximas\n",
    "for color, words in color_tree.items():\n",
    "    print(f\"\\nPalavras mais próximas da cor {color}:\")\n",
    "    for word, pmi in words:\n",
    "        print(f\"{word}: PMI = {pmi}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6359c27a-7a69-4efc-82b7-e858d034f7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir as stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Função para limpar, tokenizar e remover stop words\n",
    "def preprocess(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text.lower())  # Remove pontuação e converte para minúsculas\n",
    "    tokens = word_tokenize(text)  # Tokeniza o texto\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]  # Remove as stop words\n",
    "    return filtered_tokens\n",
    "\n",
    "# Tokenizar a coluna 'flavor' e remover as stop words\n",
    "tokenized_text = df['flavor'].apply(preprocess)\n",
    "\n",
    "# Contar as frequências das palavras\n",
    "word_freq_text = Counter(word for doc in tokenized_text for word in doc)\n",
    "\n",
    "# Contar a coocorrência de pares de palavras\n",
    "def count_coocurrences(corpus, window_size=1):\n",
    "    coocurrence = Counter()\n",
    "    for doc in corpus:\n",
    "        for i, word in enumerate(doc):\n",
    "            for j in range(i + 1, min(i + window_size + 1, len(doc))):\n",
    "                coocurrence[(word, doc[j])] += 1\n",
    "    return coocurrence\n",
    "\n",
    "coocurrence_text = count_coocurrences(tokenized_text)\n",
    "\n",
    "# Calcular o PMI para um par de palavras\n",
    "def calculate_pmi(word1, word2, coocurrence, word_freq, corpus_size):\n",
    "    coocurrence_count = coocurrence.get((word1, word2), 0)\n",
    "    p_word1 = word_freq[word1] / corpus_size\n",
    "    p_word2 = word_freq[word2] / corpus_size\n",
    "    p_word1_word2 = coocurrence_count / corpus_size\n",
    "    return np.log(p_word1_word2 / (p_word1 * p_word2)) if p_word1_word2 > 0 else 0\n",
    "\n",
    "# Calculando o PMI para todos os pares de palavras\n",
    "corpus_size_text = sum(word_freq_text.values())\n",
    "pmi_results = {}\n",
    "\n",
    "# Iterando por todos os pares únicos de palavras\n",
    "for (word1, word2), count in coocurrence_text.items():\n",
    "    pmi = calculate_pmi(word1, word2, coocurrence_text, word_freq_text, corpus_size_text)\n",
    "    pmi_results[(word1, word2)] = pmi\n",
    "\n",
    "# Exibir os resultados do PMI\n",
    "for pair, pmi_value in pmi_results.items():\n",
    "    print(f\"PMI entre {pair[0]} e {pair[1]}: {pmi_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc824d8f-0ab9-4ec3-b290-22ee9de01534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo palavras-chave para cores positivas, como exemplo\n",
    "positive_colors = ['white', 'green', 'blue']  # Branco, Verde e Azul\n",
    "\n",
    "# Função para identificar palavras associadas a cores positivas\n",
    "def get_positive_words(pmi_results, positive_colors, top_n=10):\n",
    "    positive_words = {}\n",
    "    \n",
    "    for (word1, word2), pmi in pmi_results.items():\n",
    "        if any(color in word1 for color in positive_colors) or any(color in word2 for color in positive_colors):\n",
    "            if word1 not in positive_words:\n",
    "                positive_words[word1] = pmi\n",
    "            else:\n",
    "                positive_words[word1] += pmi\n",
    "            if word2 not in positive_words:\n",
    "                positive_words[word2] = pmi\n",
    "            else:\n",
    "                positive_words[word2] += pmi\n",
    "\n",
    "    # Classificar as palavras por PMI e retornar as top_n palavras mais associadas\n",
    "    sorted_positive_words = sorted(positive_words.items(), key=lambda x: x[1], reverse=True)\n",
    "    return sorted_positive_words[:top_n]\n",
    "\n",
    "# Exemplo de como obter as top 10 palavras mais positivas\n",
    "top_positive_words = get_positive_words(pmi_results, positive_colors, top_n=20)\n",
    "print(\"Top 20 palavras com maior positividade:\")\n",
    "for word, pmi in top_positive_words:\n",
    "    print(f\"{word}: {pmi}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd8653d-adfe-4bcb-9d59-50e61eabd84d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b06277-3d30-4b82-bba6-57e448440a8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd180d61-6009-4cd8-991c-95eff02a8996",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486b8998-5d93-4b1b-8376-dd0eef12fc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Junta as palavras por cor única\n",
    "df_grouped = df.groupby('color')['processed_flavor'].apply(lambda x: ' '.join(x)).reset_index()\n",
    "\n",
    "# Inicializa o TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# Ajusta e transforma os textos agregados por cor\n",
    "tfidf_matrix = vectorizer.fit_transform(df_grouped['processed_flavor'])\n",
    "\n",
    "# Converte a matriz TF-IDF para um DataFrame para visualização\n",
    "tfidf_df = pd.DataFrame(\n",
    "    tfidf_matrix.toarray(),\n",
    "    columns=vectorizer.get_feature_names_out(),\n",
    "    index=df_grouped['color']  # Define as cores únicas como índice\n",
    ")\n",
    "\n",
    "# Exibe o DataFrame TF-IDF\n",
    "print(tfidf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1543961-69fc-49ad-9853-ec9758b2fd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empilhar os dados para todas as cores e palavras\n",
    "tfidf_data = tfidf_df.stack().reset_index()\n",
    "tfidf_data.columns = ['color', 'word', 'tfidf']\n",
    "\n",
    "# Ordenar os dados por TF-IDF de forma decrescente\n",
    "top_tfidf_all = tfidf_data.sort_values(by='tfidf', ascending=False).head(20)\n",
    "\n",
    "# Exibir o top 20\n",
    "print(top_tfidf_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81b4697-4a40-433d-b903-ad5e3e4c8f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empilhar os dados para todas as cores e palavras\n",
    "tfidf_data = tfidf_df.stack().reset_index()\n",
    "tfidf_data.columns = ['color', 'word', 'tfidf']\n",
    "\n",
    "# Ordenar os dados por TF-IDF de forma decrescente para pegar as 20 palavras mais relevantes\n",
    "top_tfidf_all = tfidf_data.sort_values(by='tfidf', ascending=False).head(20)\n",
    "\n",
    "# Pegar as 20 palavras mais relevantes\n",
    "top_20_words = top_tfidf_all['word'].unique()\n",
    "\n",
    "# Filtrar a matriz TF-IDF para as 20 palavras mais relevantes\n",
    "filtered_tfidf = tfidf_df[top_20_words]\n",
    "\n",
    "# Exibir o TF-IDF das 20 palavras para as 5 cores específicas\n",
    "top_20_tfidf = filtered_tfidf.loc[df['color'].unique()[:5]]  # Aqui você pode ajustar o número de cores ou selecionar específicas\n",
    "print(top_20_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe438d32-ecbb-4de1-9c98-4ef12bd28ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular a soma dos valores de TF-IDF para cada palavra\n",
    "tfidf_sum = tfidf_df.sum(axis=0)\n",
    "\n",
    "# Identificar palavras a remover: TF-IDF > 0.1 em todas as cores e soma > 0.6\n",
    "words_to_remove = []\n",
    "\n",
    "for word in tfidf_df.columns:\n",
    "    if all(tfidf_df[word] > 0.1) and tfidf_sum[word] > 0.6:\n",
    "        words_to_remove.append(word)\n",
    "\n",
    "# Função para remover as palavras da lista\n",
    "def remove_words(flavor_list, words_to_remove):\n",
    "    return [word for word in flavor_list if word not in words_to_remove]\n",
    "            \n",
    "words_to_remove.append('color')\n",
    "# Remover as palavras identificadas\n",
    "filtered_tfidf_df = tfidf_df.drop(columns=words_to_remove)\n",
    "\n",
    "# Aplicar a função à coluna 'processed_flavor'\n",
    "df['processed_flavor'] = df['processed_flavor'].apply(lambda x: remove_words(x, words_to_remove))\n",
    "\n",
    "# Exibir as palavras removidas\n",
    "print(\"Palavras removidas:\", words_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de6c2ec-462a-4904-973d-9ef9e32ebb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular a soma dos valores de TF-IDF para cada palavra\n",
    "tfidf_sum = filtered_tfidf_df.sum(axis=0)\n",
    "\n",
    "# Identificar palavras a remover: TF-IDF > 0.1 em todas as cores e soma > 0.6\n",
    "words_to_remove = []\n",
    "\n",
    "for word in filtered_tfidf_df.columns:\n",
    "    if all(filtered_tfidf_df[word] > 0.1) and tfidf_sum[word] > 0.6:\n",
    "        words_to_remove.append(word)\n",
    "\n",
    "# Remover as palavras identificadas\n",
    "filtered_tfidf_df_df = filtered_tfidf_df.drop(columns=words_to_remove)\n",
    "\n",
    "# Agora, reorganizar as palavras em uma coluna, mantendo as cores nas linhas\n",
    "# \"melt\" transforma as colunas em uma única coluna de palavras e uma coluna de cores\n",
    "melted_df = filtered_tfidf_df.reset_index().melt(id_vars=['color'], var_name='word', value_name='tfidf_value')\n",
    "\n",
    "# Remover as linhas com valor de TF-IDF 0 (não precisam ser exibidas)\n",
    "melted_df = melted_df[melted_df['tfidf_value'] > 0]\n",
    "\n",
    "# Exibir o DataFrame reorganizado\n",
    "print(melted_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e2d3a0c-4b6f-4d7e-835c-bb52214598b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "essafica ætheric 124teste essatambémfica\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def remove_non_letter_words(text):\n",
    "    # split do texto\n",
    "    words = text.split()\n",
    "    # retira tudo que contém números e caracteres especiais (são sejam letras) e também retira palavras com letras repetidas (algumas onomatopéias como \"aaaaahhhh\")\n",
    "    clean_words = [word for word in words if re.search(r'[a-zA-Z]', word)  and not re.search(r'(.)\\1{2,}', word)] \n",
    "    return ' '.join(clean_words)\n",
    "\n",
    "text = \"aaaah essafica  ætheric 1234 124teste essatambémfica\"\n",
    "print(remove_non_letter_words(text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
